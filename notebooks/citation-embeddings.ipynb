{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Citation embeddings\n",
    "Exploring BERT and sciBERT to predict data citations in classification and embedding tasks\n",
    "\n",
    "Findings: \n",
    "- sciBERT determines sentences from scientific articles to be more similar to each other than BERT does\n",
    "- BERT do a better job of differentiating and clustering similar citances and indicator terms\n",
    "\n",
    "References:\n",
    "\n",
    "- [Multi Class Text Classification With Deep Learning Using BERT](https://github.com/susanli2016/NLP-with-Python/blob/master/Text_Classification_With_BERT.ipynb)\n",
    "- [Domain-Specific BERT Models](https://mccormickml.com/2020/06/22/domain-specific-bert-tutorial/)\n",
    "- [sciBERT demo](https://colab.research.google.com/drive/19loLGUDjxGKy4ulZJ1m3hALq2ozNyEGe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the directory (data, models on Turbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/nfs/turbo/hrg/coleridge/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare sciBERT to BERT\n",
    "Token overlap between the vocabs ~42%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel.from_pretrained('bert-base-uncased',\n",
    "                                  output_hidden_states = True) \n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "scibert_model = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\",\n",
    "                                  output_hidden_states=True)\n",
    "\n",
    "scibert_tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# scibert_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example biomedical text and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hydrogels are hydrophilic polymer networks which may absorb from 10–20% (an arbitrary lower limit) up to thousands of times their dry weight in water.\"\n",
    "word = 'hydrogels'\n",
    "words = ['polymerization', \n",
    "         '2,2-azo-isobutyronitrile',\n",
    "         'multifunctional crosslinkers'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the sentence into tokens with both BERT and SciBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "scibert_tokens = scibert_tokenizer.tokenize(text)\n",
    "\n",
    "while len(scibert_tokens) < len(bert_tokens):\n",
    "    scibert_tokens.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT         SciBERT     \n",
      "----         -------     \n",
      "hydro        hydrogels   \n",
      "##gel        are         \n",
      "##s          hydrophilic \n",
      "are          polymer     \n",
      "hydro        networks    \n",
      "##phi        which       \n",
      "##lic        may         \n",
      "polymer      absorb      \n",
      "networks     from        \n",
      "which        10          \n",
      "may          –           \n",
      "absorb       20          \n",
      "from         %           \n",
      "10           (           \n",
      "–            an          \n",
      "20           arbitrary   \n",
      "%            lower       \n",
      "(            limit       \n",
      "an           )           \n",
      "arbitrary    up          \n",
      "lower        to          \n",
      "limit        thousands   \n",
      ")            of          \n",
      "up           times       \n",
      "to           their       \n",
      "thousands    dry         \n",
      "of           weight      \n",
      "times        in          \n",
      "their        water       \n",
      "dry          .           \n",
      "weight                   \n",
      "in                       \n",
      "water                    \n",
      ".                        \n"
     ]
    }
   ],
   "source": [
    "print('{:<12} {:<12}'.format(\"BERT\", \"SciBERT\"))\n",
    "print('{:<12} {:<12}'.format(\"----\", \"-------\"))\n",
    "\n",
    "for tup in zip(bert_tokens, scibert_tokens):\n",
    "    print('{:<12} {:<12}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_indeces(tokenizer, text, word):\n",
    "    '''\n",
    "    Determines the index or indeces of the tokens corresponding to `word`\n",
    "    within `text`. `word` can consist of multiple words, e.g., \"cell biology\".\n",
    "    \n",
    "    Determining the indeces is tricky because words can be broken into multiple\n",
    "    tokens. I've solved this with a rather roundabout approach--I replace `word`\n",
    "    with the correct number of `[MASK]` tokens, and then find these in the \n",
    "    tokenized result. \n",
    "    '''\n",
    "    # Tokenize the 'word'--it may be broken into multiple tokens or subwords.\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "\n",
    "    # Create a sequence of `[MASK]` tokens to put in place of `word`.\n",
    "    masks_str = ' '.join(['[MASK]']*len(word_tokens))\n",
    "\n",
    "    # Replace the word with mask tokens.\n",
    "    text_masked = text.replace(word, masks_str)\n",
    "\n",
    "    # `encode` performs multiple functions:\n",
    "    #   1. Tokenizes the text\n",
    "    #   2. Maps the tokens to their IDs\n",
    "    #   3. Adds the special [CLS] and [SEP] tokens.\n",
    "    input_ids = tokenizer.encode(text_masked)\n",
    "\n",
    "    # Use numpy's `where` function to find all indeces of the [MASK] token.\n",
    "    mask_token_indeces = np.where(np.array(input_ids) == tokenizer.mask_token_id)[0]\n",
    "\n",
    "    return mask_token_indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(b_model, b_tokenizer, text, word=''):\n",
    "    '''\n",
    "    Uses the provided model and tokenizer to produce an embedding for the\n",
    "    provided `text`, and a \"contextualized\" embedding for `word`, if provided.\n",
    "    '''\n",
    "\n",
    "    # If a word is provided, figure out which tokens correspond to it.\n",
    "    if not word == '':\n",
    "        word_indeces = get_word_indeces(b_tokenizer, text, word)\n",
    "\n",
    "    # Encode the text, adding the (required!) special tokens, and converting to\n",
    "    # PyTorch tensors.\n",
    "    encoded_dict = b_tokenizer.encode_plus(\n",
    "                        text,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                )\n",
    "\n",
    "    input_ids = encoded_dict['input_ids']\n",
    "    \n",
    "    b_model.eval()\n",
    "\n",
    "    # Run the text through the model and get the hidden states.\n",
    "    bert_outputs = b_model(input_ids)\n",
    "    \n",
    "    # Run the text through BERT, and collect all of the hidden states produced\n",
    "    # from all 12 layers. \n",
    "    with torch.no_grad():\n",
    "\n",
    "        outputs = b_model(input_ids)\n",
    "\n",
    "        # Evaluating the model will return a different number of objects based on \n",
    "        # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "        # becase we set `output_hidden_states = True`, the third item will be the \n",
    "        # hidden states from all layers. See the documentation for more details:\n",
    "        # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # `hidden_states` has shape [13 x 1 x <sentence length> x 768]\n",
    "\n",
    "    # Select the embeddings from the second to last layer.\n",
    "    # `token_vecs` is a tensor with shape [<sent length> x 768]\n",
    "    token_vecs = hidden_states[-2][0]\n",
    "\n",
    "    # Calculate the average of all token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "\n",
    "    # Convert to numpy array.\n",
    "    sentence_embedding = sentence_embedding.detach().numpy()\n",
    "\n",
    "    # If `word` was provided, compute an embedding for those tokens.\n",
    "    if not word == '':\n",
    "        # Take the average of the embeddings for the tokens in `word`.\n",
    "        word_embedding = torch.mean(token_vecs[word_indeces], dim=0)\n",
    "\n",
    "        # Convert to numpy array.\n",
    "        word_embedding = word_embedding.detach().numpy()\n",
    "    \n",
    "        return (sentence_embedding, word_embedding)\n",
    "    else:\n",
    "        return sentence_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the embedding for the sentence, as well as an embedding for 'hydrogels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding sizes:\n",
      "(768,)\n",
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "(sen_emb, word_emb) = get_embedding(scibert_model, scibert_tokenizer, text, word)\n",
    "\n",
    "print('Embedding sizes:')\n",
    "print(sen_emb.shape)\n",
    "print(word_emb.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity of the two embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = 1 - cosine(sen_emb, word_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence comparison examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citance and non-citance similarity (from same paper)\n",
    "- `text_query` and `text_A` are true citances but refer to different datasets\n",
    "- `text_B` does not refer to a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'query' should be more similar to 'A' than to 'B'...\n",
      "\n",
      "SciBERT:\n",
      "  sim(query, A): 0.96\n",
      "  sim(query, B): 0.89\n",
      "\n",
      "BERT:\n",
      "  sim(query, A): 0.92\n",
      "  sim(query, B): 0.73\n"
     ]
    }
   ],
   "source": [
    "# Three sentences; query is more similar to A than B.\n",
    "text_query = \"In addition to HadISST, we have analyzed four other SST datasets: the NOAA Extended Reconstructed Sea Surface Temperature version 3b (ERSST.v3b; Smith et al. 2008) , the NOAA Optimum Interpolation Sea Surface Temperature version 2 (OISSTv2; Reynolds et al. 2002) , and the Japan Meteorological Agency Centennial in situ ObservationBased Estimates (COBE; Ishii et al. 2005) \"\n",
    "text_A = \"The SST data in Fig. 15a is sourced from the Hadley Centre Sea Ice and Sea Surface Temperature dataset (HadISST; Rayner et al. 2003) , which incorporates satellite data, float, and ship measurements.\"\n",
    "text_B = \"The sudden increase of the relaxation temperature by 0.58C at year 0 increases the surface heat flux.\"\n",
    "\n",
    "# Get embeddings for each.\n",
    "emb_query = get_embedding(scibert_model, scibert_tokenizer, text_query)\n",
    "emb_A = get_embedding(scibert_model, scibert_tokenizer, text_A)\n",
    "emb_B = get_embedding(scibert_model, scibert_tokenizer, text_B)\n",
    "\n",
    "# Compare query to A and B with cosine similarity.\n",
    "sim_query_A = 1 - cosine(emb_query, emb_A)\n",
    "sim_query_B = 1 - cosine(emb_query, emb_B)\n",
    "\n",
    "print(\"'query' should be more similar to 'A' than to 'B'...\\n\")\n",
    "\n",
    "print('SciBERT:')\n",
    "print('  sim(query, A): {:.2}'.format(sim_query_A))\n",
    "print('  sim(query, B): {:.2}'.format(sim_query_B))\n",
    "\n",
    "# Repeat with BERT.\n",
    "emb_query = get_embedding(bert_model, bert_tokenizer, text_query)\n",
    "emb_A = get_embedding(bert_model, bert_tokenizer, text_A)\n",
    "emb_B = get_embedding(bert_model, bert_tokenizer, text_B)\n",
    "\n",
    "# Compare query to A and B with cosine similarity.\n",
    "sim_query_A = 1 - cosine(emb_query, emb_A)\n",
    "sim_query_B = 1 - cosine(emb_query, emb_B)\n",
    "\n",
    "print('')\n",
    "print('BERT:')\n",
    "print('  sim(query, A): {:.2}'.format(sim_query_A))\n",
    "print('  sim(query, B): {:.2}'.format(sim_query_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-bio/CS example\n",
    "- `text_query` and `text_A` come from the same article and refer to the same dataset\n",
    "- `text_B` comes from a different article and refers to a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'query' should be more similar to 'A' than to 'B'...\n",
      "\n",
      "SciBERT:\n",
      "  sim(query, A): 0.96\n",
      "  sim(query, B): 0.93\n",
      "\n",
      "BERT:\n",
      "  sim(query, A): 0.89\n",
      "  sim(query, B): 0.75\n"
     ]
    }
   ],
   "source": [
    "# Three sentences; query is more similar to A than B.\n",
    "text_query = \"For example, NASS collects data from farm and ranch operations in many surveys and the Census of Agriculture (COA).\"\n",
    "text_A = \"Also, the farm-level data collected by the Census of Agriculture is necessary to track farm transitions, such as new farmer entry (Gale, 2002) , beginning development (Ahearn & Newton, 2009) , and switching between marketing channels.\"\n",
    "text_B = \"We evaluate whether gender differences in the likelihood of obtaining a tenure track job, promotion to tenure, and promotion to full professor explain these facts using the 1973-2001 Survey of Doctorate Recipients.\"\n",
    "\n",
    "# Get embeddings for each.\n",
    "emb_query = get_embedding(scibert_model, scibert_tokenizer, text_query)\n",
    "emb_A = get_embedding(scibert_model, scibert_tokenizer, text_A)\n",
    "emb_B = get_embedding(scibert_model, scibert_tokenizer, text_B)\n",
    "\n",
    "# Compare query to A and B with cosine similarity.\n",
    "sim_query_A = 1 - cosine(emb_query, emb_A)\n",
    "sim_query_B = 1 - cosine(emb_query, emb_B)\n",
    "\n",
    "print(\"'query' should be more similar to 'A' than to 'B'...\\n\")\n",
    "\n",
    "print('SciBERT:')\n",
    "print('  sim(query, A): {:.2}'.format(sim_query_A))\n",
    "print('  sim(query, B): {:.2}'.format(sim_query_B))\n",
    "\n",
    "# Repeat with BERT.\n",
    "emb_query = get_embedding(bert_model, bert_tokenizer, text_query)\n",
    "emb_A = get_embedding(bert_model, bert_tokenizer, text_A)\n",
    "emb_B = get_embedding(bert_model, bert_tokenizer, text_B)\n",
    "\n",
    "# Compare query to A and B with cosine similarity.\n",
    "sim_query_A = 1 - cosine(emb_query, emb_A)\n",
    "sim_query_B = 1 - cosine(emb_query, emb_B)\n",
    "\n",
    "print('')\n",
    "print('BERT:')\n",
    "print('  sim(query, A): {:.2}'.format(sim_query_A))\n",
    "print('  sim(query, B): {:.2}'.format(sim_query_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biomedical example\n",
    "Sentences containing data citations; we would expect to see that query text and text A are more similar\n",
    "- `text_query` and `text_A` come from the same article and refer to the same dataset\n",
    "- `text_B` comes from a different article and refers to a different dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'query' should be more similar to 'A' than to 'B'...\n",
      "\n",
      "SciBERT:\n",
      "  sim(query, A): 0.99\n",
      "  sim(query, B): 0.93\n",
      "\n",
      "BERT:\n",
      "  sim(query, A): 0.97\n",
      "  sim(query, B): 0.84\n"
     ]
    }
   ],
   "source": [
    "text_query = \"The primary goal of ADNI has been to test whether serial MRI, positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD.\"\n",
    "text_A = \"The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimers disease (AD).\" \n",
    "text_B = \"In this study, we quantified individual species of plasma sphingomyelin and dihydrosphingomyelin in 992 individuals, aged 55 and older, enrolled in the Baltimore Longitudinal Study of Aging (BLSA).\"\n",
    "\n",
    "# Get embeddings for each.\n",
    "emb_query = get_embedding(scibert_model, scibert_tokenizer, text_query)\n",
    "emb_A = get_embedding(scibert_model, scibert_tokenizer, text_A)\n",
    "emb_B = get_embedding(scibert_model, scibert_tokenizer, text_B)\n",
    "\n",
    "# Compare query to A and B with cosine similarity.\n",
    "sim_query_A = 1 - cosine(emb_query, emb_A)\n",
    "sim_query_B = 1 - cosine(emb_query, emb_B)\n",
    "\n",
    "print(\"'query' should be more similar to 'A' than to 'B'...\\n\")\n",
    "\n",
    "print('SciBERT:')\n",
    "print('  sim(query, A): {:.2}'.format(sim_query_A))\n",
    "print('  sim(query, B): {:.2}'.format(sim_query_B))\n",
    "\n",
    "# Repeat with BERT.\n",
    "emb_query = get_embedding(bert_model, bert_tokenizer, text_query)\n",
    "emb_A = get_embedding(bert_model, bert_tokenizer, text_A)\n",
    "emb_B = get_embedding(bert_model, bert_tokenizer, text_B)\n",
    "\n",
    "# Compare query to A and B with cosine similarity.\n",
    "sim_query_A = 1 - cosine(emb_query, emb_A)\n",
    "sim_query_B = 1 - cosine(emb_query, emb_B)\n",
    "\n",
    "print('')\n",
    "print('BERT:')\n",
    "print('  sim(query, A): {:.2}'.format(sim_query_A))\n",
    "print('  sim(query, B): {:.2}'.format(sim_query_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word comparison examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The survey of agriculture is a large, long running, publically available data source.\"\n",
      "\n",
      "SciBERT:\n",
      "  sim(survey, agriculture): 0.79\n",
      "  sim(survey, data): 0.76\n",
      "\n",
      "BERT:\n",
      "  sim(survey, agriculture): 0.44\n",
      "  sim(survey, data): 0.7\n"
     ]
    }
   ],
   "source": [
    "text = \"The survey of agriculture is a large, long running, publically available data source.\"\n",
    "\n",
    "print('\"' + text + '\"\\n')\n",
    "\n",
    "# ======== SciBERT ========\n",
    "\n",
    "# Get contextualized embeddings for \"census\", \"data\", and \"agriculture\"\n",
    "(emb_sen, emb_s) = get_embedding(scibert_model, scibert_tokenizer, text, word=\"survey\")\n",
    "(emb_sen, emb_d) = get_embedding(scibert_model, scibert_tokenizer, text, word=\"data\")\n",
    "(emb_sen, emb_a) = get_embedding(scibert_model, scibert_tokenizer, text, word=\"agriculture\")\n",
    "\n",
    "# Compare the embeddings\n",
    "print('SciBERT:')\n",
    "print('  sim(survey, agriculture): {:.2}'.format((1 - cosine(emb_s, emb_a))))\n",
    "print('  sim(survey, data): {:.2}'.format(1 - cosine(emb_s, emb_d)))\n",
    "\n",
    "print('')\n",
    "\n",
    "# ======== BERT ========\n",
    "\n",
    "# Get contextualized embeddings for \"census\", \"data\", and \"agriculture\"\n",
    "(emb_sen, emb_s) = get_embedding(bert_model, bert_tokenizer, text, word=\"survey\")\n",
    "(emb_sen, emb_d) = get_embedding(bert_model, bert_tokenizer, text, word=\"data\")\n",
    "(emb_sen, emb_a) = get_embedding(bert_model, bert_tokenizer, text, word=\"agriculture\")\n",
    "\n",
    "# Compare the embeddings\n",
    "print('BERT:')\n",
    "print('  sim(survey, agriculture): {:.2}'.format((1 - cosine(emb_s, emb_a))))\n",
    "print('  sim(survey, data): {:.2}'.format(1 - cosine(emb_s, emb_d)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first see if a transformer model can predict which dataset a paper cites based on its title alone\n",
    "- This classification task assumes a 1:1 relationship between papers and datasets so for this trial, let's drop duplicate papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d0fa7568-7d8e-4db9-870f-f9c6f668c17b</td>\n",
       "      <td>The Impact of Dual Enrollment on College Degre...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2f26f645-3dec-485d-b68d-f013c9e05e60</td>\n",
       "      <td>Educational Attainment of High School Dropouts...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29</td>\n",
       "      <td>Differences in Outcomes for Female and Male St...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5c9a3bc9-41ba-4574-ad71-e25c1442c8af</td>\n",
       "      <td>Stepping Stone and Option Value in a Model of ...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>c754dec7-c5a3-4337-9892-c02158475064</td>\n",
       "      <td>Parental Effort, School Resources, and Student...</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>National Education Longitudinal Study</td>\n",
       "      <td>national education longitudinal study</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Id  \\\n",
       "0  d0fa7568-7d8e-4db9-870f-f9c6f668c17b   \n",
       "1  2f26f645-3dec-485d-b68d-f013c9e05e60   \n",
       "2  c5d5cd2c-59de-4f29-bbb1-6a88c7b52f29   \n",
       "3  5c9a3bc9-41ba-4574-ad71-e25c1442c8af   \n",
       "4  c754dec7-c5a3-4337-9892-c02158475064   \n",
       "\n",
       "                                           pub_title  \\\n",
       "0  The Impact of Dual Enrollment on College Degre...   \n",
       "1  Educational Attainment of High School Dropouts...   \n",
       "2  Differences in Outcomes for Female and Male St...   \n",
       "3  Stepping Stone and Option Value in a Model of ...   \n",
       "4  Parental Effort, School Resources, and Student...   \n",
       "\n",
       "                           dataset_title  \\\n",
       "0  National Education Longitudinal Study   \n",
       "1  National Education Longitudinal Study   \n",
       "2  National Education Longitudinal Study   \n",
       "3  National Education Longitudinal Study   \n",
       "4  National Education Longitudinal Study   \n",
       "\n",
       "                           dataset_label  \\\n",
       "0  National Education Longitudinal Study   \n",
       "1  National Education Longitudinal Study   \n",
       "2  National Education Longitudinal Study   \n",
       "3  National Education Longitudinal Study   \n",
       "4  National Education Longitudinal Study   \n",
       "\n",
       "                           cleaned_label  \n",
       "0  national education longitudinal study  \n",
       "1  national education longitudinal study  \n",
       "2  national education longitudinal study  \n",
       "3  national education longitudinal study  \n",
       "4  national education longitudinal study  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_DIR+'train.csv')\n",
    "df = df.drop_duplicates(subset=['pub_title'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes are highly imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dataset_title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Alzheimer's Disease Neuroimaging Initiative (ADNI)                                             3797\n",
       "Baltimore Longitudinal Study of Aging (BLSA)                                                   1157\n",
       "Trends in International Mathematics and Science Study                                          1113\n",
       "Early Childhood Longitudinal Study                                                              949\n",
       "SARS-CoV-2 genome sequence                                                                      735\n",
       "Agricultural Resource Management Survey                                                         641\n",
       "Census of Agriculture                                                                           592\n",
       "Rural-Urban Continuum Codes                                                                     486\n",
       "Survey of Earned Doctorates                                                                     412\n",
       "NOAA Tide Gauge                                                                                 398\n",
       "North American Breeding Bird Survey (BBS)                                                       378\n",
       "World Ocean Database                                                                            307\n",
       "Sea, Lake, and Overland Surges from Hurricanes                                                  306\n",
       "Education Longitudinal Study                                                                    305\n",
       "National Education Longitudinal Study                                                           294\n",
       "International Best Track Archive for Climate Stewardship                                        280\n",
       "Coastal Change Analysis Program                                                                 272\n",
       "Common Core of Data                                                                             228\n",
       "Beginning Postsecondary Student                                                                 221\n",
       "Our World in Data COVID-19 dataset                                                              210\n",
       "Optimum Interpolation Sea Surface Temperature                                                   185\n",
       "Survey of Doctorate Recipients                                                                  160\n",
       "Baccalaureate and Beyond                                                                        155\n",
       "National Assessment of Education Progress                                                       121\n",
       "COVID-19 Open Research Dataset (CORD-19)                                                        120\n",
       "Survey of Industrial Research and Development                                                    81\n",
       "High School Longitudinal Study                                                                   61\n",
       "Program for the International Assessment of Adult Competencies                                   61\n",
       "Survey of Graduate Students and Postdoctorates in Science and Engineering                        43\n",
       "COVID-19 Image Data Collection                                                                   38\n",
       "School Survey on Crime and Safety                                                                30\n",
       "National Teacher and Principal Survey                                                            29\n",
       "COVID-19 Deaths data                                                                             28\n",
       "Advanced National Seismic System (ANSS) Comprehensive Catalog (ComCat)                           23\n",
       "Higher Education Research and Development Survey                                                 17\n",
       "The National Institute on Aging Genetics of Alzheimer's Disease Data Storage Site (NIAGADS)      14\n",
       "Characterizing Health Associated Risks, and Your Baseline Disease In SARS-COV-2 (CHARYBDIS)       5\n",
       "Survey of Science and Engineering Research Facilities                                             5\n",
       "CAS COVID-19 antiviral candidate compounds dataset                                                3\n",
       "RSNA International COVID-19 Open Radiology Database (RICORD)                                      3\n",
       "COVID-19 Precision Medicine Analytics Platform Registry (JH-CROWN)                                2\n",
       "Aging Integrated Database (AGID)                                                                  2\n",
       "FFRDC Research and Development Survey                                                             2\n",
       "Complexity Science Hub COVID-19 Control Strategies List (CCCSL)                                   2\n",
       "Name: dataset_title, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['dataset_title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'National Education Longitudinal Study': 0,\n",
       " 'NOAA Tide Gauge': 1,\n",
       " 'Sea, Lake, and Overland Surges from Hurricanes': 2,\n",
       " 'Coastal Change Analysis Program': 3,\n",
       " 'Aging Integrated Database (AGID)': 4,\n",
       " \"Alzheimer's Disease Neuroimaging Initiative (ADNI)\": 5,\n",
       " 'Baltimore Longitudinal Study of Aging (BLSA)': 6,\n",
       " 'Agricultural Resource Management Survey': 7,\n",
       " 'Beginning Postsecondary Student': 8,\n",
       " \"The National Institute on Aging Genetics of Alzheimer's Disease Data Storage Site (NIAGADS)\": 9,\n",
       " 'Common Core of Data': 10,\n",
       " 'Survey of Industrial Research and Development': 11,\n",
       " 'Baccalaureate and Beyond': 12,\n",
       " 'International Best Track Archive for Climate Stewardship': 13,\n",
       " 'National Teacher and Principal Survey': 14,\n",
       " 'Higher Education Research and Development Survey': 15,\n",
       " 'Survey of Earned Doctorates': 16,\n",
       " 'School Survey on Crime and Safety': 17,\n",
       " 'World Ocean Database': 18,\n",
       " 'Program for the International Assessment of Adult Competencies': 19,\n",
       " 'Early Childhood Longitudinal Study': 20,\n",
       " 'Survey of Graduate Students and Postdoctorates in Science and Engineering': 21,\n",
       " 'Trends in International Mathematics and Science Study': 22,\n",
       " 'Education Longitudinal Study': 23,\n",
       " 'Optimum Interpolation Sea Surface Temperature': 24,\n",
       " 'National Assessment of Education Progress': 25,\n",
       " 'High School Longitudinal Study': 26,\n",
       " 'Survey of Doctorate Recipients': 27,\n",
       " 'Rural-Urban Continuum Codes': 28,\n",
       " 'Survey of Science and Engineering Research Facilities': 29,\n",
       " 'FFRDC Research and Development Survey': 30,\n",
       " 'Advanced National Seismic System (ANSS) Comprehensive Catalog (ComCat)': 31,\n",
       " 'Census of Agriculture': 32,\n",
       " 'North American Breeding Bird Survey (BBS)': 33,\n",
       " 'COVID-19 Open Research Dataset (CORD-19)': 34,\n",
       " 'Complexity Science Hub COVID-19 Control Strategies List (CCCSL)': 35,\n",
       " 'Our World in Data COVID-19 dataset': 36,\n",
       " 'COVID-19 Precision Medicine Analytics Platform Registry (JH-CROWN)': 37,\n",
       " 'Characterizing Health Associated Risks, and Your Baseline Disease In SARS-COV-2 (CHARYBDIS)': 38,\n",
       " 'COVID-19 Deaths data': 39,\n",
       " 'SARS-CoV-2 genome sequence': 40,\n",
       " 'COVID-19 Image Data Collection': 41,\n",
       " 'RSNA International COVID-19 Open Radiology Database (RICORD)': 42,\n",
       " 'CAS COVID-19 antiviral candidate compounds dataset': 43}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "possible_labels = df.dataset_title.unique()\n",
    "\n",
    "label_dict = {}\n",
    "for index, possible_label in enumerate(possible_labels):\n",
    "    label_dict[possible_label] = index\n",
    "\n",
    "label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19654</th>\n",
       "      <td>f89dd9fa-07af-4384-aa0c-0d14602c0cea</td>\n",
       "      <td>Artificial Intelligence of COVID-19 Imaging: A...</td>\n",
       "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
       "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
       "      <td>rsna international covid 19 open radiology dat...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19656</th>\n",
       "      <td>b3498176-8832-4033-aea6-b5ea85ea04c4</td>\n",
       "      <td>RSNA International Trends: A Global Perspectiv...</td>\n",
       "      <td>RSNA International COVID-19 Open Radiology Dat...</td>\n",
       "      <td>RSNA International COVID Open Radiology Database</td>\n",
       "      <td>rsna international covid open radiology database</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19657</th>\n",
       "      <td>f77eb51f-c3ac-420b-9586-cb187849c321</td>\n",
       "      <td>MCCS: a novel recognition pattern-based method...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19658</th>\n",
       "      <td>ab59bcdd-7b7c-4107-93f5-0ccaf749236c</td>\n",
       "      <td>Quantitative Structure–Activity Relationship M...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19659</th>\n",
       "      <td>fd23e7e0-a5d2-4f98-992d-9209c85153bb</td>\n",
       "      <td>A ligand-based computational drug repurposing ...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>CAS COVID-19 antiviral candidate compounds dat...</td>\n",
       "      <td>cas covid 19 antiviral candidate compounds dat...</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Id  \\\n",
       "19654  f89dd9fa-07af-4384-aa0c-0d14602c0cea   \n",
       "19656  b3498176-8832-4033-aea6-b5ea85ea04c4   \n",
       "19657  f77eb51f-c3ac-420b-9586-cb187849c321   \n",
       "19658  ab59bcdd-7b7c-4107-93f5-0ccaf749236c   \n",
       "19659  fd23e7e0-a5d2-4f98-992d-9209c85153bb   \n",
       "\n",
       "                                               pub_title  \\\n",
       "19654  Artificial Intelligence of COVID-19 Imaging: A...   \n",
       "19656  RSNA International Trends: A Global Perspectiv...   \n",
       "19657  MCCS: a novel recognition pattern-based method...   \n",
       "19658  Quantitative Structure–Activity Relationship M...   \n",
       "19659  A ligand-based computational drug repurposing ...   \n",
       "\n",
       "                                           dataset_title  \\\n",
       "19654  RSNA International COVID-19 Open Radiology Dat...   \n",
       "19656  RSNA International COVID-19 Open Radiology Dat...   \n",
       "19657  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19658  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19659  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "\n",
       "                                           dataset_label  \\\n",
       "19654  RSNA International COVID-19 Open Radiology Dat...   \n",
       "19656   RSNA International COVID Open Radiology Database   \n",
       "19657  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19658  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "19659  CAS COVID-19 antiviral candidate compounds dat...   \n",
       "\n",
       "                                           cleaned_label  label  \n",
       "19654  rsna international covid 19 open radiology dat...     42  \n",
       "19656   rsna international covid open radiology database     42  \n",
       "19657  cas covid 19 antiviral candidate compounds dat...     43  \n",
       "19658  cas covid 19 antiviral candidate compounds dat...     43  \n",
       "19659  cas covid 19 antiviral candidate compounds dat...     43  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.dataset_title.replace(label_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation split\n",
    "Labels are imbalanced so stratify the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>pub_title</th>\n",
       "      <th>dataset_label</th>\n",
       "      <th>cleaned_label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dataset_title</th>\n",
       "      <th>label</th>\n",
       "      <th>data_type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Advanced National Seismic System (ANSS) Comprehensive Catalog (ComCat)</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">31</th>\n",
       "      <th>train</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Aging Integrated Database (AGID)</th>\n",
       "      <th>4</th>\n",
       "      <th>train</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Agricultural Resource Management Survey</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">7</th>\n",
       "      <th>train</th>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "      <td>545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The National Institute on Aging Genetics of Alzheimer's Disease Data Storage Site (NIAGADS)</th>\n",
       "      <th>9</th>\n",
       "      <th>val</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Trends in International Mathematics and Science Study</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">22</th>\n",
       "      <th>train</th>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "      <td>946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">World Ocean Database</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">18</th>\n",
       "      <th>train</th>\n",
       "      <td>261</td>\n",
       "      <td>261</td>\n",
       "      <td>261</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>val</th>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     Id  \\\n",
       "dataset_title                                      label data_type        \n",
       "Advanced National Seismic System (ANSS) Compreh... 31    train       20   \n",
       "                                                         val          3   \n",
       "Aging Integrated Database (AGID)                   4     train        2   \n",
       "Agricultural Resource Management Survey            7     train      545   \n",
       "                                                         val         96   \n",
       "...                                                                 ...   \n",
       "The National Institute on Aging Genetics of Alz... 9     val          2   \n",
       "Trends in International Mathematics and Science... 22    train      946   \n",
       "                                                         val        167   \n",
       "World Ocean Database                               18    train      261   \n",
       "                                                         val         46   \n",
       "\n",
       "                                                                    pub_title  \\\n",
       "dataset_title                                      label data_type              \n",
       "Advanced National Seismic System (ANSS) Compreh... 31    train             20   \n",
       "                                                         val                3   \n",
       "Aging Integrated Database (AGID)                   4     train              2   \n",
       "Agricultural Resource Management Survey            7     train            545   \n",
       "                                                         val               96   \n",
       "...                                                                       ...   \n",
       "The National Institute on Aging Genetics of Alz... 9     val                2   \n",
       "Trends in International Mathematics and Science... 22    train            946   \n",
       "                                                         val              167   \n",
       "World Ocean Database                               18    train            261   \n",
       "                                                         val               46   \n",
       "\n",
       "                                                                    dataset_label  \\\n",
       "dataset_title                                      label data_type                  \n",
       "Advanced National Seismic System (ANSS) Compreh... 31    train                 20   \n",
       "                                                         val                    3   \n",
       "Aging Integrated Database (AGID)                   4     train                  2   \n",
       "Agricultural Resource Management Survey            7     train                545   \n",
       "                                                         val                   96   \n",
       "...                                                                           ...   \n",
       "The National Institute on Aging Genetics of Alz... 9     val                    2   \n",
       "Trends in International Mathematics and Science... 22    train                946   \n",
       "                                                         val                  167   \n",
       "World Ocean Database                               18    train                261   \n",
       "                                                         val                   46   \n",
       "\n",
       "                                                                    cleaned_label  \n",
       "dataset_title                                      label data_type                 \n",
       "Advanced National Seismic System (ANSS) Compreh... 31    train                 20  \n",
       "                                                         val                    3  \n",
       "Aging Integrated Database (AGID)                   4     train                  2  \n",
       "Agricultural Resource Management Survey            7     train                545  \n",
       "                                                         val                   96  \n",
       "...                                                                           ...  \n",
       "The National Institute on Aging Genetics of Alz... 9     val                    2  \n",
       "Trends in International Mathematics and Science... 22    train                946  \n",
       "                                                         val                  167  \n",
       "World Ocean Database                               18    train                261  \n",
       "                                                         val                   46  \n",
       "\n",
       "[82 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df.index.values, \n",
    "                                                  df.label.values, \n",
    "                                                  test_size=0.15, \n",
    "                                                  random_state=42, \n",
    "                                                  stratify=df.label.values)\n",
    "\n",
    "df['data_type'] = ['not_set']*df.shape[0]\n",
    "\n",
    "df.loc[X_train, 'data_type'] = 'train'\n",
    "df.loc[X_val, 'data_type'] = 'val'\n",
    "\n",
    "df.groupby(['dataset_title', 'label', 'data_type']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and data encoding\n",
    "We will use a pre-trained BERT model configuration to encode our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5cf51fe78643a5b3db58f478e78a4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d07eae7e9d14349826ab1fca27fa833",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be76da6e367446f0b4848f79de97f656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', \n",
    "                                          do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='train'].pub_title.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "#     truncation=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    df[df.data_type=='val'].pub_title.values, \n",
    "    add_special_tokens=True, \n",
    "    return_attention_mask=True, \n",
    "    pad_to_max_length=True, \n",
    "    max_length=256, \n",
    "#     truncation=True,\n",
    "    return_tensors='pt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df[df.data_type=='train'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df[df.data_type=='val'].label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_train), len(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Pre-trained model\n",
    "Treat each `pub_title` as a unique sequence so one sequence will be classified to one of the 44 `dataset_label`\n",
    "\n",
    "- bert-base-uncased is a smaller pre-trained model;\n",
    "- num_labels indicates the number of output labels;\n",
    "- we don’t care about output_attentions;\n",
    "- we also don’t need output_hidden_states\n",
    "\n",
    "About the warning:\n",
    "https://github.com/huggingface/transformers/issues/5421"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "#                                                       num_labels=len(label_dict),\n",
    "#                                                       output_attentions=False,\n",
    "#                                                       output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, \n",
    "                              sampler=RandomSampler(dataset_train), \n",
    "                              batch_size=batch_size)\n",
    "\n",
    "dataloader_validation = DataLoader(dataset_val, \n",
    "                                   sampler=SequentialSampler(dataset_val), \n",
    "                                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer and scheduler\n",
    "Consider changing the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=1e-5, \n",
    "                  eps=1e-8)\n",
    "                  \n",
    "epochs = 5\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score_func(preds, labels):\n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average='weighted')\n",
    "\n",
    "def accuracy_per_class(preds, labels):\n",
    "    label_dict_inverse = {v: k for k, v in label_dict.items()}\n",
    "    \n",
    "    preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    for label in np.unique(labels_flat):\n",
    "        y_preds = preds_flat[labels_flat==label]\n",
    "        y_true = labels_flat[labels_flat==label]\n",
    "        print(f'Class: {label_dict_inverse[label]}')\n",
    "        print(f'Accuracy: {len(y_preds[y_preds==label])}/{len(y_true)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 17\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "    \n",
    "    for batch in dataloader_val:\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "        loss = outputs[0]\n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "    \n",
    "    loss_val_avg = loss_val_total/len(dataloader_val) \n",
    "    \n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "            \n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f6e61ad1e749faacf994142af2d98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd12481a0384da5982b61af3626aa30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch 1', max=4044.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, epochs+1)):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    loss_train_total = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "    for batch in progress_bar:\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        \n",
    "        inputs = {'input_ids':      batch[0],\n",
    "                  'attention_mask': batch[1],\n",
    "                  'labels':         batch[2],\n",
    "                 }       \n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss_train_total += loss.item()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "         \n",
    "        \n",
    "    torch.save(model.state_dict(), f'data_volume/finetuned_BERT_epoch_{epoch}.model')\n",
    "        \n",
    "    tqdm.write(f'\\nEpoch {epoch}')\n",
    "    \n",
    "    loss_train_avg = loss_train_total/len(dataloader_train)            \n",
    "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "    \n",
    "    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n",
    "    val_f1 = f1_score_func(predictions, true_vals)\n",
    "    tqdm.write(f'Validation loss: {val_loss}')\n",
    "    tqdm.write(f'F1 Score (Weighted): {val_f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
    "                                                      num_labels=len(label_dict),\n",
    "                                                      output_attentions=False,\n",
    "                                                      output_hidden_states=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('data_volume/finetuned_BERT_epoch_1.model', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, predictions, true_vals = evaluate(dataloader_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_per_class(predictions, true_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
